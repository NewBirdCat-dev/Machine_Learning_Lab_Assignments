{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1) Implement decision tree algorithm on a dataset of your choice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "{'worst radius <= 16.86': [{'mean concave points <= 0.0734': [1, 0]}, 0]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def entropy(y):\n",
    "    if isinstance(y, pd.Series):\n",
    "        a = y.value_counts() / y.shape[0]\n",
    "        entropy_value = np.sum(-a * np.log2(a + 1e-9))\n",
    "        return entropy_value\n",
    "    else:\n",
    "        raise ValueError('Object must be a Pandas Series.')\n",
    "\n",
    "def information_gain(y, mask, func=entropy):\n",
    "    y_left = y[mask]\n",
    "    y_right = y[~mask]\n",
    "    p_left = len(y_left) / len(y)\n",
    "    p_right = len(y_right) / len(y)\n",
    "    return func(y) - (p_left * func(y_left) + p_right * func(y_right))\n",
    "\n",
    "def categorical_options(a):\n",
    "    a = a.unique()\n",
    "    opciones = []\n",
    "    for L in range(0, len(a) + 1):\n",
    "        for subset in itertools.combinations(a, L):\n",
    "            opciones.append(list(subset))\n",
    "    return opciones[1:-1]\n",
    "\n",
    "def max_information_gain_split(x, y, func=entropy):\n",
    "    split_value = []\n",
    "    gain = []\n",
    "    numeric_variable = x.dtypes != 'O'\n",
    "\n",
    "    if numeric_variable:\n",
    "        options = x.sort_values().unique()[1:]\n",
    "    else:\n",
    "        options = categorical_options(x)\n",
    "\n",
    "    for val in options:\n",
    "        mask = x < val if numeric_variable else x.isin(val)\n",
    "        val_gain = information_gain(y, mask, func)\n",
    "        gain.append(val_gain)\n",
    "        split_value.append(val)\n",
    "\n",
    "    if len(gain) == 0:\n",
    "        return (None, None, None, False)\n",
    "    else:\n",
    "        best_gain = max(gain)\n",
    "        best_gain_index = gain.index(best_gain)\n",
    "        best_split = split_value[best_gain_index]\n",
    "        return (best_gain, best_split, numeric_variable, True)\n",
    "\n",
    "def get_best_split(y, dataset):\n",
    "    masks = dataset.drop(y, axis=1).apply(max_information_gain_split, y=dataset[y])\n",
    "    if sum(masks.loc[3, :]) == 0:\n",
    "        return (None, None, None, None)\n",
    "    else:\n",
    "        masks = masks.loc[:, masks.loc[3, :]]\n",
    "        split_variable = masks.iloc[0].astype(np.float32).idxmax()\n",
    "        split_value = masks[split_variable][1]\n",
    "        split_gain = masks[split_variable][0]\n",
    "        split_numeric = masks[split_variable][2]\n",
    "        return (split_variable, split_value, split_gain, split_numeric)\n",
    "\n",
    "def make_split(variable, value, dataset, is_numeric):\n",
    "    if is_numeric:\n",
    "        dataset_1 = dataset[dataset[variable] < value]\n",
    "        dataset_2 = dataset[(dataset[variable] < value) == False]\n",
    "    else:\n",
    "        dataset_1 = dataset[dataset[variable].isin(value)]\n",
    "        dataset_2 = dataset[(dataset[variable].isin(value)) == False]\n",
    "    return (dataset_1, dataset_2)\n",
    "\n",
    "def make_prediction(dataset, target_factor):\n",
    "    if target_factor:\n",
    "        pred = dataset.value_counts().idxmax()\n",
    "    else:\n",
    "        pred = dataset.mean()\n",
    "    return pred\n",
    "\n",
    "def train_tree(dataset, y, target_factor, max_depth=None, min_samples_split=None, min_information_gain=1e-20, counter=0, max_categories=20):\n",
    "    if counter == 0:\n",
    "        types = dataset.dtypes\n",
    "        check_columns = types[types == \"object\"].index\n",
    "\n",
    "    depth_cond = counter < max_depth if max_depth is not None else True\n",
    "    sample_cond = dataset.shape[0] > min_samples_split if min_samples_split is not None else True\n",
    "\n",
    "    if depth_cond and sample_cond:\n",
    "        var, val, gain, var_type = get_best_split(y, dataset)\n",
    "\n",
    "        if gain is not None and gain >= min_information_gain:\n",
    "            counter += 1\n",
    "            left, right = make_split(var, val, dataset, var_type)\n",
    "\n",
    "            split_type = \"<=\" if var_type else \"in\"\n",
    "            question = \"{} {} {}\".format(var, split_type, val)\n",
    "            subtree = {question: []}\n",
    "\n",
    "            yes_answer = train_tree(left, y, target_factor, max_depth, min_samples_split, min_information_gain, counter, max_categories)\n",
    "            no_answer = train_tree(right, y, target_factor, max_depth, min_samples_split, min_information_gain, counter, max_categories)\n",
    "\n",
    "            if yes_answer == no_answer:\n",
    "                subtree = yes_answer\n",
    "            else:\n",
    "                subtree[question].append(yes_answer)\n",
    "                subtree[question].append(no_answer)\n",
    "\n",
    "        else:\n",
    "            return make_prediction(dataset[y], target_factor)\n",
    "    else:\n",
    "        return make_prediction(dataset[y], target_factor)\n",
    "\n",
    "    return subtree\n",
    "\n",
    "dataset = pd.read_csv('breast_cancer_dataset.csv')\n",
    "\n",
    "dataset = dataset.sample(n=20, random_state=1)\n",
    "\n",
    "max_depth = 5\n",
    "min_samples_split = 2\n",
    "min_information_gain = 1e-5\n",
    "\n",
    "decision_tree = train_tree(dataset, 'target', True, max_depth, min_samples_split, min_information_gain)\n",
    "\n",
    "print(\"Decision Tree Structure:\")\n",
    "print(decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2) Instead of Entropy, use GINI INDEX and observe the performance for any difference(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "{'worst radius <= 16.86': [{'mean concave points <= 0.0734': [1, 0]}, 0]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def gini(y):\n",
    "    if isinstance(y, pd.Series):\n",
    "        a = y.value_counts() / y.shape[0]\n",
    "        gini_value = 1 - np.sum(a ** 2)\n",
    "        return gini_value\n",
    "    else:\n",
    "        raise ValueError('Object must be a Pandas Series.')\n",
    "\n",
    "def gini_information_gain(y, mask):\n",
    "    y_left = y[mask]\n",
    "    y_right = y[~mask]\n",
    "    p_left = len(y_left) / len(y)\n",
    "    p_right = len(y_right) / len(y)\n",
    "    return gini(y) - (p_left * gini(y_left) + p_right * gini(y_right))\n",
    "\n",
    "def max_gini_split(x, y):\n",
    "    split_value = []\n",
    "    gain = []\n",
    "    numeric_variable = x.dtypes != 'O'\n",
    "\n",
    "    if numeric_variable:\n",
    "        options = x.sort_values().unique()[1:]\n",
    "    else:\n",
    "        options = categorical_options(x)\n",
    "\n",
    "    for val in options:\n",
    "        mask = x < val if numeric_variable else x.isin(val)\n",
    "        val_gain = gini_information_gain(y, mask)\n",
    "        gain.append(val_gain)\n",
    "        split_value.append(val)\n",
    "\n",
    "    if len(gain) == 0:\n",
    "        return (None, None, None, False)\n",
    "    else:\n",
    "        best_gain = max(gain)\n",
    "        best_gain_index = gain.index(best_gain)\n",
    "        best_split = split_value[best_gain_index]\n",
    "        return (best_gain, best_split, numeric_variable, True)\n",
    "\n",
    "def get_best_split(y, dataset):\n",
    "    masks = dataset.drop(y, axis=1).apply(max_gini_split, y=dataset[y])\n",
    "    if sum(masks.loc[3, :]) == 0:\n",
    "        return (None, None, None, None)\n",
    "    else:\n",
    "        masks = masks.loc[:, masks.loc[3, :]]\n",
    "        split_variable = masks.iloc[0].astype(np.float32).idxmax()\n",
    "        split_value = masks[split_variable][1]\n",
    "        split_gain = masks[split_variable][0]\n",
    "        split_numeric = masks[split_variable][2]\n",
    "        return (split_variable, split_value, split_gain, split_numeric)\n",
    "\n",
    "def make_split(variable, value, dataset, is_numeric):\n",
    "    if is_numeric:\n",
    "        dataset_1 = dataset[dataset[variable] < value]\n",
    "        dataset_2 = dataset[(dataset[variable] < value) == False]\n",
    "    else:\n",
    "        dataset_1 = dataset[dataset[variable].isin(value)]\n",
    "        dataset_2 = dataset[(dataset[variable].isin(value)) == False]\n",
    "    return (dataset_1, dataset_2)\n",
    "\n",
    "def make_prediction(dataset, target_factor):\n",
    "    if target_factor:\n",
    "        pred = dataset.value_counts().idxmax()\n",
    "    else:\n",
    "        pred = dataset.mean()\n",
    "    return pred\n",
    "\n",
    "def train_tree(dataset, y, target_factor, max_depth=None, min_samples_split=None, min_information_gain=1e-20, counter=0, max_categories=20):\n",
    "    if counter == 0:\n",
    "        types = dataset.dtypes\n",
    "        check_columns = types[types == \"object\"].index\n",
    "\n",
    "    depth_cond = counter < max_depth if max_depth is not None else True\n",
    "    sample_cond = dataset.shape[0] > min_samples_split if min_samples_split is not None else True\n",
    "\n",
    "    if depth_cond and sample_cond:\n",
    "        var, val, gain, var_type = get_best_split(y, dataset)\n",
    "\n",
    "        if gain is not None and gain >= min_information_gain:\n",
    "            counter += 1\n",
    "            left, right = make_split(var, val, dataset, var_type)\n",
    "\n",
    "            split_type = \"<=\" if var_type else \"in\"\n",
    "            question = \"{} {} {}\".format(var, split_type, val)\n",
    "            subtree = {question: []}\n",
    "\n",
    "            yes_answer = train_tree(left, y, target_factor, max_depth, min_samples_split, min_information_gain, counter, max_categories)\n",
    "            no_answer = train_tree(right, y, target_factor, max_depth, min_samples_split, min_information_gain, counter, max_categories)\n",
    "\n",
    "            if yes_answer == no_answer:\n",
    "                subtree = yes_answer\n",
    "            else:\n",
    "                subtree[question].append(yes_answer)\n",
    "                subtree[question].append(no_answer)\n",
    "\n",
    "        else:\n",
    "            return make_prediction(dataset[y], target_factor)\n",
    "    else:\n",
    "        return make_prediction(dataset[y], target_factor)\n",
    "\n",
    "    return subtree\n",
    "\n",
    "dataset = pd.read_csv('breast_cancer_dataset.csv')\n",
    "dataset = dataset.sample(n=20, random_state=1)\n",
    "\n",
    "max_depth = 5\n",
    "min_samples_split = 2\n",
    "min_information_gain = 1e-5\n",
    "\n",
    "decision_tree = train_tree(dataset, 'target', True, max_depth, min_samples_split, min_information_gain)\n",
    "\n",
    "print(\"Decision Tree Structure:\")\n",
    "print(decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3) Employ SciKit Learn implementation of decision tree and observe for any difference(s). Try different types of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with criterion='gini', max_depth=None, min_samples_split=2:\n",
      "[[37  5]\n",
      " [ 1 71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.93        42\n",
      "           1       0.93      0.99      0.96        72\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.93      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Decision Tree with criterion='gini', max_depth=3, min_samples_split=2:\n",
      "[[36  6]\n",
      " [ 4 68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88        42\n",
      "           1       0.92      0.94      0.93        72\n",
      "\n",
      "    accuracy                           0.91       114\n",
      "   macro avg       0.91      0.90      0.90       114\n",
      "weighted avg       0.91      0.91      0.91       114\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Decision Tree with criterion='entropy', max_depth=None, min_samples_split=2:\n",
      "[[36  6]\n",
      " [ 0 72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        42\n",
      "           1       0.92      1.00      0.96        72\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.96      0.93      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Decision Tree with criterion='entropy', max_depth=3, min_samples_split=2:\n",
      "[[37  5]\n",
      " [ 5 67]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        42\n",
      "           1       0.93      0.93      0.93        72\n",
      "\n",
      "    accuracy                           0.91       114\n",
      "   macro avg       0.91      0.91      0.91       114\n",
      "weighted avg       0.91      0.91      0.91       114\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Decision Tree with criterion='gini', max_depth=5, min_samples_split=5:\n",
      "[[37  5]\n",
      " [ 1 71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.93        42\n",
      "           1       0.93      0.99      0.96        72\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.93      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Decision Tree with criterion='entropy', max_depth=5, min_samples_split=5:\n",
      "[[37  5]\n",
      " [ 0 72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.94        42\n",
      "           1       0.94      1.00      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.94      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dataset = pd.read_csv('breast_cancer_dataset.csv')\n",
    "\n",
    "X = dataset.drop(columns=['target'])\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "tree_configs = [\n",
    "    {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2},\n",
    "    {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2},\n",
    "    {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2},\n",
    "    {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2},\n",
    "    {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5},\n",
    "    {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5},\n",
    "]\n",
    "\n",
    "for config in tree_configs:\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion=config['criterion'],\n",
    "        max_depth=config['max_depth'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        random_state=1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"Decision Tree with criterion='{config['criterion']}', max_depth={config['max_depth']}, min_samples_split={config['min_samples_split']}:\")\n",
    "    print(\"Decision Tree Structure:\")\n",
    "    print(decision_tree)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP38b7r+HXFho1QBiUujX5F",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
